{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21621f14-b80d-4df1-87d3-fafb269a4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU transformers  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af19b85-dd2c-4bfd-a099-4a0f34d67008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import modeling_utils\n",
    "# if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "#     modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\",\"colwise\",'rowwise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5368f1f-9ef7-433b-8dfc-3877a1340c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6a4b5526af4106a9c579c3549e4aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T07:10:58.888265+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-06-17T07:10:59.317169+0000 | _calibrate | INFO - Running QuantizationModifier calibration with 512 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [09:02<00:00,  1.06s/it]\n",
      "manager stage: Modifiers initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T07:20:01.499123+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "manager stage: Modifiers finalized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T07:20:01.500051+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
      "2025-06-17T07:20:01.500477+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "2025-06-17T07:20:01.517008+0000 | <module> | INFO - Running sample generation. \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "# Select model and load it.\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Select calibration dataset.\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "\n",
    "# Select number of samples. 512 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "\n",
    "# Load dataset and preprocess.\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "\n",
    "def process_and_tokenize(example):\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "ds = ds.map(process_and_tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "# Configure the quantization algorithm and scheme.\n",
    "# In this case, we:\n",
    "#   * quantize the weights to fp8 with per-tensor scales\n",
    "#   * quantize the activations to fp8 with per-tensor scales\n",
    "#   * quantize the kv cache to fp8 with per-tensor scales\n",
    "recipe = \"\"\"\n",
    "quant_stage:\n",
    "    quant_modifiers:\n",
    "        QuantizationModifier:\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    weights:\n",
    "                        num_bits: 8\n",
    "                        type: float\n",
    "                        strategy: tensor\n",
    "                        dynamic: false\n",
    "                        symmetric: true\n",
    "                    input_activations:\n",
    "                        num_bits: 8\n",
    "                        type: float\n",
    "                        strategy: tensor\n",
    "                        dynamic: false\n",
    "                        symmetric: true\n",
    "                    targets: [\"Linear\"]\n",
    "            kv_cache_scheme:\n",
    "                num_bits: 8\n",
    "                type: float\n",
    "                strategy: tensor\n",
    "                dynamic: false\n",
    "                symmetric: true\n",
    "\"\"\"\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Running sample generation. \",\n",
    "    \"Note: Inference with the quantized kv_cache is not supported. \",\n",
    "    \"Please use vLLM for inference with the quantized kv_cache.\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75c1c7-5149-4a01-902a-435b0becccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "========== SAMPLE GENERATION ==============\n",
      "<|begin_of_text|>Hello my name is Karen and I am a professional photographer based in the beautiful county of Kent. I have been a photographer for over 10 years and have a passion for capturing special moments in people's lives.\n",
      "I have a relaxed and friendly approach to my work, which helps to put my clients at ease and allows me to capture their true personalities and emotions. I am always looking for new and creative ways to capture the perfect shot, and I am constantly updating my skills and knowledge to stay ahead of the game.\n",
      "\n",
      "I\n",
      "==========================================\n",
      "\n",
      "\n",
      "2025-06-17T07:20:58.095490+0000 | save_pretrained_wrapper | INFO - Fetching state_dict - this may take some time\n",
      "2025-06-17T07:21:18.163962+0000 | save_pretrained_wrapper | INFO - Fetching compressor\n",
      "2025-06-17T07:21:18.164647+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantized Compression: 100%|██████████| 1251/1251 [00:10<00:00, 117.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T07:21:28.799025+0000 | save_pretrained_wrapper | INFO - Saving compressed model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm generations of the quantized model look sane.\n",
    "print(\"\\n\\n\")\n",
    "print(\"========== SAMPLE GENERATION ==============\")\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\\n\")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-FP8-KV\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
