{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9bd9ac-1848-4844-b7e4-bfb175556f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e643083f544eaf998801d58c30408c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c03c5da72e34343ba40b7c68caa20b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2c0567a83a4be99038b2bffc1c0f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be3e4bc61714f2aa5a6910c432e1df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9769e98ad6a84df5851e9c8e0bdcaca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca3b14ad8d541678f0c562ebc34a8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T09:27:25.417707+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-06-17T09:27:25.418942+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2025-06-17T09:27:26.965032+0000 | _set_resolved_mappings | INFO - Excluded 32 from resolved mappings due to shape mismatch\n",
      "2025-06-17T09:27:27.397784+0000 | _calibrate | INFO - Running AWQModifier calibration with 256 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 55/256 [00:31<02:34,  1.30it/s]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "\n",
    "# Select model and load it.\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Select calibration dataset.\n",
    "DATASET_ID = \"mit-han-lab/pile-val-backup\"\n",
    "DATASET_SPLIT = \"validation\"\n",
    "\n",
    "# Select number of samples. 256 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 256\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "# Load dataset and preprocess.\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": example[\"text\"]}],\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "\n",
    "# Tokenize inputs.\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure the quantization algorithm to run.\n",
    "recipe = [\n",
    "    AWQModifier(ignore=[\"lm_head\"], scheme=\"W4A16_ASYM\", targets=[\"Linear\"]),\n",
    "]\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1737dd2-c42f-4509-bad1-d6e0e61900d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm generations of the quantized model look sane.\n",
    "print(\"\\n\\n\")\n",
    "print(\"========== SAMPLE GENERATION ==============\")\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\\n\")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-awq-asym\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
