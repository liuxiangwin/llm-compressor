{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e6993a-24ec-442a-a963-89da2996e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import modeling_utils\n",
    "# if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "#     modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\",\"colwise\",'rowwise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014fdb40-f1d6-465d-a567-5dcb6789dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba438038dc8a48839b76f95cd7412e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\"\"\"\n",
    "This example covers how to load a quantized model using AutoModelForCausalLM.\n",
    "\n",
    "During inference, each layer will be decompressed as needed before the forward pass.\n",
    "This saves memory as only a single layer is ever uncompressed at a time, but increases\n",
    "runtime as we need to decompress each layer before running the forward pass\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# any model with the \"compressed-tensors\" quant_method and \"compressed\"\n",
    "# quantization_status in the quantization config is supported\n",
    "MODEL_STUB = \"nm-testing/tinyllama-fp8-dynamic-compressed\"\n",
    "\n",
    "SAMPLE_INPUT = [\n",
    "    \"I love quantization because\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"def fibonacci(n):\",\n",
    "]\n",
    "\n",
    "compressed_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_STUB,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad6c33b-c8da-4e6c-b774-53121a085c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee74544f6ef4da3b5fe73c4ef10781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a3560d138e447cacb8bea088945be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682b6fc91a3947f9b9121629f1bf31e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68382e53b75f4ac8b8c49460bc2e470f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== SAMPLE GENERATION ==============\n",
      "<s> I love quantization because</s></s> <br>\n",
      "    it makes the data more compact and easier to store. <br>\n",
      "    <br>\n",
      "    <b>What is the difference between quantization and compression?</b>\n",
      "<s> What is the capital of France?\n",
      "What is the capital of France?\n",
      "What is the capital of France? The capital of France is Paris.\n",
      "What is the capital of France? The capital of France is Paris.\n",
      "What is the\n",
      "<s> def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) +\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sample data\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_STUB)\n",
    "inputs = tokenizer(SAMPLE_INPUT, return_tensors=\"pt\", padding=True).to(\n",
    "    compressed_model.device\n",
    ")\n",
    "\n",
    "# run the compressed model and decode the output\n",
    "output = compressed_model.generate(**inputs, max_length=50)\n",
    "print(\"========== SAMPLE GENERATION ==============\")\n",
    "text_output = tokenizer.batch_decode(output)\n",
    "for sample in text_output:\n",
    "    print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
