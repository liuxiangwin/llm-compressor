{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21621f14-b80d-4df1-87d3-fafb269a4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af19b85-dd2c-4bfd-a099-4a0f34d67008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import modeling_utils\n",
    "if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "    modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\",\"colwise\",'rowwise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5368f1f-9ef7-433b-8dfc-3877a1340c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd7e37a0df3445a971b7f358e264809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7877cc10d0514f1d9fbe8229fcf9e99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa604a871494c878f3d8296492539de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116e40600ffb4de88ff3b1cf4d007db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e528e900c7d4116bf6db64a89bd93cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef25cc463b342dc9fc216eabcf9bc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31fab0a6c8b4066b843ed26f70eb524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00000-of-00003-a3ecf92756(…):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8dbddbdf694ef595c8934e0d4a0e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00001-of-00003-0a1804bcb6(…):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516299e1d0ff4abdaad16f5bca083512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00002-of-00003-ee46ed25cf(…):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4d37652cd14a23ba691bb54315ae64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_sft-00000-of-00001-f7dfac4afe5(…):   0%|          | 0.00/81.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6a882210ed44e5aa09cab0b4fc381b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00000-of-00003-a6c9fb894b(…):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3751fb7728a4a6c9e56737edbdf1ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00001-of-00003-d6a0402e41(…):   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43836a11fef447ef819a336a14564ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00002-of-00003-c0db75b92a(…):   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f934e8c62b3d4337b186e1eb77b0b389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_gen-00000-of-00001-3d4cd830914(…):   0%|          | 0.00/80.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15bb894fc8744f98ef1549ac30e9dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_sft split:   0%|          | 0/207865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df22784e777f4e8c98c1f0e02d80b0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_sft split:   0%|          | 0/23110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4002344644614455841375b7530b67e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_gen split:   0%|          | 0/256032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae5fe3cb2014e4b9c1cc8114f97cb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_gen split:   0%|          | 0/28304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9d4703800f4fc7a1f0d809a06efb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T05:35:58.078554+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-06-17T05:35:58.507660+0000 | _calibrate | INFO - Running QuantizationModifier calibration with 512 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [09:03<00:00,  1.06s/it]\n",
      "manager stage: Modifiers initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T05:45:01.661584+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "manager stage: Modifiers finalized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T05:45:01.662341+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
      "2025-06-17T05:45:01.662727+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "2025-06-17T05:45:01.679148+0000 | <module> | INFO - Running sample generation. \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "# Select model and load it.\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Select calibration dataset.\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "\n",
    "# Select number of samples. 512 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "\n",
    "# Load dataset and preprocess.\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "\n",
    "def process_and_tokenize(example):\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "ds = ds.map(process_and_tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "# Configure the quantization algorithm and scheme.\n",
    "# In this case, we:\n",
    "#   * quantize the weights to fp8 with per-tensor scales\n",
    "#   * quantize the activations to fp8 with per-tensor scales\n",
    "#   * quantize the kv cache to fp8 with per-tensor scales\n",
    "recipe = \"\"\"\n",
    "quant_stage:\n",
    "    quant_modifiers:\n",
    "        QuantizationModifier:\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    weights:\n",
    "                        num_bits: 8\n",
    "                        type: float\n",
    "                        strategy: tensor\n",
    "                        dynamic: false\n",
    "                        symmetric: true\n",
    "                    input_activations:\n",
    "                        num_bits: 8\n",
    "                        type: float\n",
    "                        strategy: tensor\n",
    "                        dynamic: false\n",
    "                        symmetric: true\n",
    "                    targets: [\"Linear\"]\n",
    "            kv_cache_scheme:\n",
    "                num_bits: 8\n",
    "                type: float\n",
    "                strategy: tensor\n",
    "                dynamic: false\n",
    "                symmetric: true\n",
    "\"\"\"\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Running sample generation. \",\n",
    "    \"Note: Inference with the quantized kv_cache is not supported. \",\n",
    "    \"Please use vLLM for inference with the quantized kv_cache.\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce75c1c7-5149-4a01-902a-435b0becccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "========== SAMPLE GENERATION ==============\n",
      "<|begin_of_text|>Hello my name is Kaitlyn and I am a 5th grade student at a public school in the United States. I am excited to be a part of this project and to learn more about the world and its cultures.\n",
      "I am 11 years old and I live with my family in a small town in the Midwest. I have one younger brother and we both love to play soccer and ride our bikes. My favorite subjects in school are math and science, but I also enjoy reading and writing.\n",
      "I am looking\n",
      "==========================================\n",
      "\n",
      "\n",
      "2025-06-17T05:45:58.821092+0000 | save_pretrained_wrapper | INFO - Fetching state_dict - this may take some time\n",
      "2025-06-17T05:46:12.910709+0000 | save_pretrained_wrapper | INFO - Fetching compressor\n",
      "2025-06-17T05:46:12.911520+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantized Compression: 100%|██████████| 1251/1251 [00:09<00:00, 125.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17T05:46:22.880038+0000 | save_pretrained_wrapper | INFO - Saving compressed model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DTensor' from 'torch.distributed.tensor' (/opt/app-root/lib64/python3.11/site-packages/torch/distributed/tensor/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save to disk compressed.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m SAVE_DIR \u001b[38;5;241m=\u001b[39m MODEL_ID\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-FP8-KV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_compressed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(SAVE_DIR)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/llmcompressor/transformers/sparsification/compressed_tensors_utils.py:125\u001b[0m, in \u001b[0;36mmodify_save_pretrained.<locals>.save_pretrained_compressed.<locals>.save_pretrained_wrapper\u001b[0;34m(save_directory, sparsity_config, quantization_format, save_compressed, safe_serialization, skip_sparsity_compression_stats, disable_sparse_compression, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     compressed_state_dict \u001b[38;5;241m=\u001b[39m compressor\u001b[38;5;241m.\u001b[39mcompress(model, state_dict)\n\u001b[1;32m    124\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving compressed model to disk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m     \u001b[43moriginal_save_pretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompressed_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     compressor\u001b[38;5;241m.\u001b[39mupdate_config(save_directory)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# update existing recipe\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/modeling_utils.py:3572\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;66;03m# Sometimes in the state_dict we have non-tensor objects.\u001b[39;00m\n\u001b[1;32m   3570\u001b[0m     \u001b[38;5;66;03m# e.g. in bitsandbytes we have some `str` objects in the state_dict\u001b[39;00m\n\u001b[1;32m   3571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[0;32m-> 3572\u001b[0m         ptrs[\u001b[43mid_tensor_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m   3573\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3574\u001b[0m         \u001b[38;5;66;03m# In the non-tensor case, fall back to the pointer of the object itself\u001b[39;00m\n\u001b[1;32m   3575\u001b[0m         ptrs[\u001b[38;5;28mid\u001b[39m(tensor)]\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/transformers/pytorch_utils.py:300\u001b[0m, in \u001b[0;36mid_tensor_storage\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03mUnique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03mexample, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03mguaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03mnon-overlapping lifetimes may have the same id.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_2_0:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTensor\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[1;32m    303\u001b[0m         local_tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto_local()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DTensor' from 'torch.distributed.tensor' (/opt/app-root/lib64/python3.11/site-packages/torch/distributed/tensor/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Confirm generations of the quantized model look sane.\n",
    "print(\"\\n\\n\")\n",
    "print(\"========== SAMPLE GENERATION ==============\")\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\\n\")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-FP8-KV\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
